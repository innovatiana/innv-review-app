# ğŸ§  AI Dataset QA Review Tool

This small app helps **review and validate datasets annotated for AI training**, across multiple modalities and formats. It performs **automatic quality checks** and highlights potential issues in the data, including:

- Text classification and NER datasets
- LLM fine-tuning (prompt/response) datasets
- Object detection datasets (bounding boxes, COCO-style)
- Audio/video timestamp-based annotations
- Multimodal metadata (JSON, JSONL, XML, CSV)

---

## ğŸš€ Features

### âœ… General Quality Checks
- Missing values
- Duplicate rows
- Label distribution and class imbalance

### ğŸ§  Machine Learning Checks
- Cleanlab-based anomaly detection
- Model-based prediction vs label mismatch

### âœï¸ NLP & NER Checks
- Entity span overlap detection
- Conflicting labels on same span
- Text length vs entity density
- Language mismatch (via langdetect)

### ğŸ¤– LLM Prompt/Response QA
- Missing prompt or response
- Long/short responses
- Semantic similarity scoring (via sentence-transformers)

### ğŸ•’ Timestamp QA
- Format validation (ISO 8601, hh:mm:ss, float)
- `start_time < end_time`
- Overlapping segments
- Timestamps beyond media duration (coming soon)

### ğŸ–¼ï¸ Object Detection QA
- Invalid bounding boxes
- Bboxes outside image boundaries

---

## ğŸ“‚ Supported Formats

You can upload:
- `.csv`
- `.json`
- `.jsonl`
- `.xml`
- `.zip` (with supported metadata files inside)

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/your-org/your-repo-name.git
cd your-repo-name
pip install -r requirements.txt
streamlit run main.py
